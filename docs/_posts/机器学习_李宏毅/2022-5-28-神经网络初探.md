---
title: 神经网络初探
category: 机器学习_李宏毅
tags: [机器学习]
---

# 把函数抽象为神经元

关于什么是神经元，这里不再多说，我们假设读者高中生物掌握得还不错。

神经元和函数很相似，多个输入，一个输出，输入通过某种规则来决定输出，所以自然而然，我们可以把函数抽象为神经元。

这样的做法看起来好像很自然——把人的智能和机器的智能形式上统一起来。

很多神经元相互连接，也就是所谓的神经网络。

到这里，我们就可以把神经网络的许多结构和函数的结构画上等号：

- 函数的复合（也就是函数的函数）可以看作一个神经元的输出连另一个神经元的输出。
- 函数的加法（本质也是函数的符合）就是两个函数作为输入，构成了一个二元复合函数，其实就是两个神经元的输出连到了第三个神经元的输出。

如果你已经学过了线性回归，你应该已经了解了，机器学习其实就是使用一个函数去拟合现有的数据（我们先不考虑过拟合的情况），而神经网络其实就是用一个层层符合的函数来拟合数据，我们之所以这样做的原因就是复合函数往往能够拟合出十分复杂的曲线，而且对输入更加敏感。

# 一个神经元的神经网络

我们假设有这样一个线性回归的假设函数：

$$
y = \theta_1 x_1 + \theta_2 x_2 + b
$$

这里有两个输入，三个参数，我们可以把它抽象成一个神经网络：

![](https://ym-tuchuang.oss-cn-hangzhou.aliyuncs.com/20220528215515.png)

这就是一个最简单的神经网络了。

# 激活函数

上面我们构造了一个极其简单的神经网络，我们可以通过增加更多并行的神经元来让我们拟合的函数更加复杂，但是你会发现，如果我们只用上面的神经元，最终构造出来神经网络，拟合的终究只是一个线性函数。

这个时候，我们就需要激活函数来让我们的神经网络出现一些非线性的变化。就像真正的神经元一样，它的输出往往并不是一个真正的线性的值，而是和数字电路一样的离散值。

如果你已经学过了 Logistic 回归，那么你对 Sigmoid 函数一定不陌生，这就是一种激活函数，从它的图形也可以看出这是一个非线性函数。

你可以把激活函数单独作为一个神经元，但是我们通常会把它放在神经元里面，也就是说，一个神经元其实是一个线性函数和一个非线性函数的复合。

![](https://ym-tuchuang.oss-cn-hangzhou.aliyuncs.com/20220528221911.png)

你会发现，这其实就是 Logistic 回归的假设函数。

当然，我们还可以使用很多其他的激活函数，比如 ReLU，从而让我们的神经网络拟合的函数变得异常复杂。

# 反向传播

刚才我们从输入向前计算输出的过程，叫做正向传播（前向传播）。

我们知道，机器学习的最终目标就是找到一组参数，让我们得到拟合效果最好的函数，在线性回归中我们已经学过了梯度下降和正规方程两种方法来寻找参数，事实上，梯度下降也适用于神经网络。

## 一个神经元的例子

我们首先来回忆一下梯度下降的更新公式：

$$\begin{aligned}
\theta_i :&= \theta_i - \alpha \frac{\partial}{\partial{\theta_i}}\mathrm{J}(\theta) \\
\frac{\partial}{\partial{\theta_i}}\mathrm{J}(\theta) &= \frac{2}{n} \sum_{i=1}^{n} (y(x^{(i)}) - \hat{y}^{(i)}) \frac{\partial{y_{x^{(i)}}(\theta)}}{\partial{\theta_i}}
\end{aligned}$$

所以我们只要求 $\frac{\partial}{\partial{\theta_i}}y_{x^{(i)}}(\theta)$ 就行了。

![](https://ym-tuchuang.oss-cn-hangzhou.aliyuncs.com/20220529162142.png)

## 多输入多输出的神经元

对于多层的神经元来说，其实只不过多了一个复合函数的求导——也就是链式法则。

![](https://ym-tuchuang.oss-cn-hangzhou.aliyuncs.com/20220529180309.png)

下面是通过链式法则求导数的过程：

![](https://ym-tuchuang.oss-cn-hangzhou.aliyuncs.com/20220529180432.png)

所以我们求导的过程和正向传播正好反过来，我们必须从最后一层开始逐层向输入层求导数，这也就是所谓的反向传播。